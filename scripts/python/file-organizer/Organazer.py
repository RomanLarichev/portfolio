import hashlib
import json
import logging
import shutil
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional


def setup_logging():
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è"""
    log_file = Path.home() / 'Downloads' / 'file_organizer.log'
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler()
        ]
    )


def get_file_hash(file_path: Path, buffer_size: int = 65536) -> str:
    """
    –í—ã—á–∏—Å–ª—è–µ—Ç MD5 —Ö–µ—à —Ñ–∞–π–ª–∞ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è —Ç–æ—á–Ω—ã—Ö –¥—É–±–ª–∏–∫–∞—Ç–æ–≤.
    """
    md5_hash = hashlib.md5()

    try:
        with open(file_path, 'rb') as f:
            while chunk := f.read(buffer_size):
                md5_hash.update(chunk)
    except (IOError, OSError) as e:
        logging.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã—á–∏—Å–ª–∏—Ç—å —Ö–µ—à –¥–ª—è {file_path}: {e}")
        return ""

    return md5_hash.hexdigest()


def get_category_mappings() -> Dict[str, List[str]]:
    """–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–π"""
    return {
        'Images': ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.svg', '.webp', '.tiff'],
        'Documents': ['.pdf', '.docx', '.doc', '.txt', '.rtf', '.xlsx', '.xls', '.pptx', '.odt', '.odp', '.ods'],
        'Archives': ['.zip', '.rar', '.7z', '.tar', '.gz', '.bz2', '.xz'],
        'Scripts': ['.py', '.js', '.java', '.cpp', '.c', '.h', '.html', '.css', '.php', '.sh', '.bat'],
        'Videos': ['.mp4', '.avi', '.mov', '.mkv', '.wmv', '.flv', '.webm', '.m4v'],
        'Audio': ['.mp3', '.wav', '.flac', '.aac', '.ogg', '.m4a', '.wma'],
        'Executables': ['.exe', '.msi', '.dmg', '.apk', '.app', '.deb', '.rpm'],
        'Fonts': ['.ttf', '.otf', '.woff', '.woff2', '.eot'],
        'Data': ['.csv', '.json', '.xml', '.sql', '.db', '.sqlite', '.yaml', '.yml'],
        'Presentations': ['.ppt', '.pptx', '.key'],
        'Ebooks': ['.epub', '.mobi', '.azw3'],
        'Torrents': ['.torrent'],
        'Certificates': ['.pem', '.crt', '.key', '.cer', '.pfx'],
        'Configs': ['.ini', '.cfg', '.conf', '.properties']
    }


class FileOrganizer:
    def __init__(self, source_folder: Optional[Path] = None, dry_run: bool = False):
        self.source_folder = source_folder or Path.home() / 'Downloads'
        self.dry_run = dry_run
        self.categories = get_category_mappings()
        self.stats = {
            'processed': 0,
            'moved': 0,
            'renamed': 0,
            'duplicates_found': 0,
            'duplicates_removed': 0,
            'errors': 0,
            'skipped': 0
        }
        self.hash_cache = {}  # –ö—ç—à –¥–ª—è —Ö–µ—à–µ–π —Ñ–∞–π–ª–æ–≤
        self.name_cache = {}  # –ö—ç—à –¥–ª—è –∏–º–µ–Ω —Ñ–∞–π–ª–æ–≤ (–±–µ–∑ —É—á–µ—Ç–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞)

    def get_unique_filename(self, file_path: Path, target_folder: Path) -> Optional[Path]:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞, –µ—Å–ª–∏ —Ñ–∞–π–ª —Å —Ç–∞–∫–∏–º –∏–º–µ–Ω–µ–º —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Ç—å —Å —É–Ω–∏–∫–∞–ª—å–Ω—ã–º –∏–º–µ–Ω–µ–º.
        """
        original_name = file_path.name
        name_parts = original_name.rsplit('.', 1)
        base_name = name_parts[0]
        extension = f".{name_parts[1]}" if len(name_parts) > 1 else ""

        counter = 1
        new_name = original_name
        new_path = target_folder / new_name

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞
        while new_path.exists():
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ñ–∞–π–ª —Ç–æ—á–Ω—ã–º –¥—É–±–ª–∏–∫–∞—Ç–æ–º
            if self.is_exact_duplicate(file_path, new_path):
                logging.info(f"–ù–∞–π–¥–µ–Ω —Ç–æ—á–Ω—ã–π –¥—É–±–ª–∏–∫–∞—Ç: {file_path.name} -> {new_path.name}")
                self.stats['duplicates_found'] += 1

                # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –Ω–∞—Å—Ç—Ä–æ–µ–∫
                if not self.dry_run:
                    try:
                        file_path.unlink()
                        self.stats['duplicates_removed'] += 1
                        logging.info(f"–£–¥–∞–ª–µ–Ω –¥—É–±–ª–∏–∫–∞—Ç: {file_path.name}")
                    except Exception as e:
                        logging.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å –¥—É–±–ª–∏–∫–∞—Ç {file_path.name}: {e}")
                        self.stats['errors'] += 1
                else:
                    logging.info(f"[DRY RUN] –ë—ã–ª –±—ã —É–¥–∞–ª–µ–Ω –¥—É–±–ª–∏–∫–∞—Ç: {file_path.name}")

                return None  # –§–∞–π–ª-–¥—É–±–ª–∏–∫–∞—Ç, –Ω–µ –Ω—É–∂–Ω–æ –ø–µ—Ä–µ–º–µ—â–∞—Ç—å

            # –ï—Å–ª–∏ –Ω–µ –¥—É–±–ª–∏–∫–∞—Ç, –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –Ω–æ–≤–æ–µ –∏–º—è
            new_name = f"{base_name}_{counter}{extension}"
            new_path = target_folder / new_name
            counter += 1

            # –ó–∞—â–∏—Ç–∞ –æ—Ç –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ–≥–æ —Ü–∏–∫–ª–∞
            if counter > 100:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
                new_name = f"{base_name}_{timestamp}{extension}"
                new_path = target_folder / new_name
                break

        return new_path

    def is_exact_duplicate(self, file1: Path, file2: Path) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è—é—Ç—Å—è –ª–∏ –¥–≤–∞ —Ñ–∞–π–ª–∞ —Ç–æ—á–Ω—ã–º–∏ –¥—É–±–ª–∏–∫–∞—Ç–∞–º–∏.
        –°–Ω–∞—á–∞–ª–∞ —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç —Ä–∞–∑–º–µ—Ä, –∑–∞—Ç–µ–º —Ö–µ—à —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ.
        """
        # –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ —Ä–∞–∑–º–µ—Ä—É
        try:
            if file1.stat().st_size != file2.stat().st_size:
                return False
        except (OSError, IOError):
            return False

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ —Ö–µ—à—É
        hash1 = self.hash_cache.get(file1)
        if hash1 is None:
            hash1 = get_file_hash(file1)
            self.hash_cache[file1] = hash1

        hash2 = self.hash_cache.get(file2)
        if hash2 is None:
            hash2 = get_file_hash(file2)
            self.hash_cache[file2] = hash2

        return hash1 and hash2 and hash1 == hash2

    @staticmethod
    def check_case_insensitive_duplicate(filename: str, target_folder: Path) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ —Ñ–∞–π–ª–∞ —Å —Ç–µ–º –∂–µ –∏–º–µ–Ω–µ–º –±–µ–∑ —É—á–µ—Ç–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞.
        –í–∞–∂–Ω–æ –¥–ª—è Windows/Linux —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏.
        """
        for existing_file in target_folder.iterdir():
            if existing_file.is_file() and existing_file.name.lower() == filename.lower():
                return True
        return False

    def create_category_folders(self):
        """–°–æ–∑–¥–∞–µ—Ç –ø–∞–ø–∫–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–π, –µ—Å–ª–∏ –æ–Ω–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç"""
        for category in self.categories.keys():
            category_path = self.source_folder / category
            if not category_path.exists():
                if not self.dry_run:
                    category_path.mkdir(parents=True, exist_ok=True)
                    logging.info(f"–°–æ–∑–¥–∞–Ω–∞ –ø–∞–ø–∫–∞: {category}")
                else:
                    logging.info(f"[DRY RUN] –ë—ã–ª–∞ –±—ã —Å–æ–∑–¥–∞–Ω–∞ –ø–∞–ø–∫–∞: {category}")
            else:
                logging.debug(f"–ü–∞–ø–∫–∞ —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {category}")

    def organize_files(self) -> bool:
        """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ —Ñ–∞–π–ª–æ–≤"""
        if not self.source_folder.exists():
            logging.error(f"–ü–∞–ø–∫–∞ '{self.source_folder}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")
            return False

        # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        self.create_category_folders()

        # –°–Ω–∞—á–∞–ª–∞ —Å–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        files_to_process = []
        for item in self.source_folder.iterdir():
            if item.is_file() and not item.name.startswith('.') and item.suffix != '.log':
                files_to_process.append(item)

        self.stats['processed'] = len(files_to_process)

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ–∞–π–ª—ã
        for file_path in files_to_process:
            self.process_file(file_path)

        return True

    def process_file(self, file_path: Path):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω —Ñ–∞–π–ª"""
        file_ext = file_path.suffix.lower()
        moved = False

        for category, extensions in self.categories.items():
            if file_ext in extensions:
                target_folder = self.source_folder / category

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏ –ø–æ–ª—É—á–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω–æ–µ –∏–º—è
                unique_path = self.get_unique_filename(file_path, target_folder)

                if unique_path is None:
                    # –§–∞–π–ª –±—ã–ª —É–¥–∞–ª–µ–Ω –∫–∞–∫ –¥—É–±–ª–∏–∫–∞—Ç
                    moved = True
                    break

                # –ï—Å–ª–∏ —Ñ–∞–π–ª —Å —Ç–∞–∫–∏–º –∏–º–µ–Ω–µ–º —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç (–±–µ–∑ —É—á–µ—Ç–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞)
                if self.check_case_insensitive_duplicate(unique_path.name, target_folder):
                    logging.warning(f"–§–∞–π–ª —Å –ø–æ—Ö–æ–∂–∏–º –∏–º–µ–Ω–µ–º (—Ä–µ–≥–∏—Å—Ç—Ä) —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {unique_path.name}")

                # –ü–µ—Ä–µ–º–µ—â–∞–µ–º —Ñ–∞–π–ª
                try:
                    if not self.dry_run:
                        shutil.move(str(file_path), str(unique_path))
                        self.stats['moved'] += 1

                        if file_path.name != unique_path.name:
                            self.stats['renamed'] += 1
                            logging.info(
                                f"–ü–µ—Ä–µ–º–µ—â–µ–Ω —Å –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ–º: {file_path.name} -> {category}/{unique_path.name}")
                        else:
                            logging.info(f"–ü–µ—Ä–µ–º–µ—â–µ–Ω: {file_path.name} -> {category}/")
                    else:
                        if file_path.name != unique_path.name:
                            logging.info(
                                f"[DRY RUN] –ë—ã–ª –±—ã –ø–µ—Ä–µ–º–µ—â–µ–Ω —Å –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ–º: {file_path.name} -> {category}/{unique_path.name}")
                        else:
                            logging.info(f"[DRY RUN] –ë—ã–ª –±—ã –ø–µ—Ä–µ–º–µ—â–µ–Ω: {file_path.name} -> {category}/")

                    moved = True
                    break

                except Exception as e:
                    logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–µ—Ä–µ–º–µ—â–µ–Ω–∏–∏ {file_path.name}: {e}")
                    self.stats['errors'] += 1
                    moved = True  # –ü–æ–º–µ—á–∞–µ–º –∫–∞–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π, –¥–∞–∂–µ —Å –æ—à–∏–±–∫–æ–π
                    break

        if not moved:
            # –§–∞–π–ª –Ω–µ –ø–æ–¥–æ—à–µ–ª –Ω–∏ –ø–æ–¥ –æ–¥–Ω—É –∫–∞—Ç–µ–≥–æ—Ä–∏—é
            logging.debug(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç: {file_path.name} ({file_ext})")
            self.stats['skipped'] += 1

    def find_all_duplicates(self, recursive: bool = True) -> Dict[str, List[Path]]:
        """
        –ù–∞—Ö–æ–¥–∏—Ç –≤—Å–µ –¥—É–±–ª–∏–∫–∞—Ç—ã —Ñ–∞–π–ª–æ–≤ –≤ –∏—Å—Ö–æ–¥–Ω–æ–π –ø–∞–ø–∫–µ.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å: —Ö–µ—à -> —Å–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ —Ñ–∞–π–ª–∞–º
        """
        duplicates = {}

        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã
        if recursive:
            all_files = list(self.source_folder.rglob('*'))
        else:
            all_files = list(self.source_folder.iterdir())

        all_files = [f for f in all_files if f.is_file() and not f.name.startswith('.')]

        # –í—ã—á–∏—Å–ª—è–µ–º —Ö–µ—à–∏
        logging.info(f"–ü–æ–∏—Å–∫ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ —Å—Ä–µ–¥–∏ {len(all_files)} —Ñ–∞–π–ª–æ–≤...")

        for file_path in all_files:
            try:
                file_hash = get_file_hash(file_path)
                if file_hash:
                    if file_hash not in duplicates:
                        duplicates[file_hash] = []
                    duplicates[file_hash].append(file_path)
            except Exception as e:
                logging.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ñ–∞–π–ª {file_path}: {e}")

        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –¥—É–±–ª–∏–∫–∞—Ç—ã (—Ö–µ—à–∏ —Å –±–æ–ª–µ–µ —á–µ–º –æ–¥–Ω–∏–º —Ñ–∞–π–ª–æ–º)
        return {h: files for h, files in duplicates.items() if len(files) > 1}

    def remove_duplicates(self, keep_oldest: bool = True):
        """
        –£–¥–∞–ª—è–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã —Ñ–∞–π–ª–æ–≤, –æ—Å—Ç–∞–≤–ª—è—è —Ç–æ–ª—å–∫–æ –æ–¥–Ω—É –∫–æ–ø–∏—é.
        –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –æ—Å—Ç–∞–≤–ª—è–µ—Ç —Å–∞–º—É—é —Å—Ç–∞—Ä—É—é –≤–µ—Ä—Å–∏—é.
        """
        duplicates = self.find_all_duplicates()

        if not duplicates:
            logging.info("–î—É–±–ª–∏–∫–∞—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")
            return

        total_duplicates = sum(len(files) - 1 for files in duplicates.values())
        logging.info(f"–ù–∞–π–¥–µ–Ω–æ {len(duplicates)} –≥—Ä—É–ø–ø –¥—É–±–ª–∏–∫–∞—Ç–æ–≤, –≤—Å–µ–≥–æ {total_duplicates} —Ñ–∞–π–ª–æ–≤-–¥—É–±–ª–∏–∫–∞—Ç–æ–≤")

        removed_count = 0

        for file_hash, files in duplicates.items():
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Ñ–∞–π–ª—ã –ø–æ –≤—Ä–µ–º–µ–Ω–∏ —Å–æ–∑–¥–∞–Ω–∏—è
            files_with_mtime = [(f, f.stat().st_mtime) for f in files]
            files_with_mtime.sort(key=lambda x: x[1])

            # –û—Å—Ç–∞–≤–ª—è–µ–º –ø–µ—Ä–≤—ã–π —Ñ–∞–π–ª (—Å–∞–º—ã–π —Å—Ç–∞—Ä—ã–π –∏–ª–∏ —Å–∞–º—ã–π –Ω–æ–≤—ã–π –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –Ω–∞—Å—Ç—Ä–æ–µ–∫)
            if not keep_oldest:
                files_with_mtime = files_with_mtime[::-1]  # –†–µ–≤–µ—Ä—Å–∏—Ä—É–µ–º, —á—Ç–æ–±—ã –æ—Å—Ç–∞–≤–∏—Ç—å —Å–∞–º—ã–π –Ω–æ–≤—ã–π

            file_to_keep = files_with_mtime[0][0]

            # –£–¥–∞–ª—è–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã
            for file_path, _ in files_with_mtime[1:]:
                try:
                    if not self.dry_run:
                        file_path.unlink()
                        removed_count += 1
                        logging.info(f"–£–¥–∞–ª–µ–Ω –¥—É–±–ª–∏–∫–∞—Ç: {file_path.name} (–æ—Ä–∏–≥–∏–Ω–∞–ª: {file_to_keep.name})")
                    else:
                        logging.info(
                            f"[DRY RUN] –ë—ã–ª –±—ã —É–¥–∞–ª–µ–Ω –¥—É–±–ª–∏–∫–∞—Ç: {file_path.name} (–æ—Ä–∏–≥–∏–Ω–∞–ª: {file_to_keep.name})")
                except Exception as e:
                    logging.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å –¥—É–±–ª–∏–∫–∞—Ç {file_path.name}: {e}")

        logging.info(f"–£–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {removed_count}")
        self.stats['duplicates_removed'] += removed_count

    def print_statistics(self):
        """–í—ã–≤–æ–¥–∏—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ä–∞–±–æ—Ç—ã"""
        print("\n" + "=" * 60)
        print("–°–¢–ê–¢–ò–°–¢–ò–ö–ê –û–†–ì–ê–ù–ò–ó–ê–¶–ò–ò:")
        print("=" * 60)

        stats_items = [
            ("–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ñ–∞–π–ª–æ–≤", self.stats['processed']),
            ("–£—Å–ø–µ—à–Ω–æ –ø–µ—Ä–µ–º–µ—â–µ–Ω–æ", self.stats['moved']),
            ("–ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–æ", self.stats['renamed']),
            ("–ù–∞–π–¥–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤", self.stats['duplicates_found']),
            ("–£–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤", self.stats['duplicates_removed']),
            ("–ü—Ä–æ–ø—É—â–µ–Ω–æ", self.stats['skipped']),
            ("–û—à–∏–±–æ–∫", self.stats['errors'])
        ]

        for name, value in stats_items:
            print(f"{name:25} | {value:5}")

        if self.stats['processed'] > 0:
            success_rate = ((self.stats['moved'] + self.stats['duplicates_removed']) /
                            self.stats['processed']) * 100
            print(f"{'–£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ:':25} | {success_rate:5.1f}%")

        print("=" * 60)

    def save_report(self, report_path: Optional[Path] = None):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ—Ç—á–µ—Ç –≤ JSON —Ñ–∞–π–ª"""
        if report_path is None:
            report_path = self.source_folder / 'organization_report.json'

        report = {
            'timestamp': datetime.now().isoformat(),
            'source_folder': str(self.source_folder),
            'dry_run': self.dry_run,
            'statistics': self.stats,
            'categories': {k: len(v) for k, v in self.categories.items()}
        }

        if not self.dry_run:
            with open(report_path, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            logging.info(f"–û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_path}")


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    setup_logging()

    print("üóÇÔ∏è  –û–†–ì–ê–ù–ò–ó–ê–¢–û–† –§–ê–ô–õ–û–í")
    print("=" * 40)

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏
    source_folder = Path.home() / 'Downloads'
    print(f"–ò—Å—Ö–æ–¥–Ω–∞—è –ø–∞–ø–∫–∞: {source_folder}")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –ø–∞–ø–∫–∏
    if not source_folder.exists():
        print(f"‚ùå –ü–∞–ø–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {source_folder}")
        return

    # –í—ã–±–æ—Ä —Ä–µ–∂–∏–º–∞
    dry_run_input = input("\n–ó–∞–ø—É—Å—Ç–∏—Ç—å –≤ —Ä–µ–∂–∏–º–µ –ø—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä–∞? (y/n): ").lower()
    dry_run = dry_run_input == 'y'

    if dry_run:
        print("\nüîç –†–ï–ñ–ò–ú –ü–†–ï–î–ü–†–û–°–ú–û–¢–†–ê - —Ñ–∞–π–ª—ã –Ω–µ –±—É–¥—É—Ç –ø–µ—Ä–µ–º–µ—â–µ–Ω—ã")

    # –í—ã–±–æ—Ä –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –æ–ø—Ü–∏–π
    remove_dups_input = input("\n–ò—Å–∫–∞—Ç—å –∏ —É–¥–∞–ª—è—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã? (y/n): ").lower()
    remove_duplicates = remove_dups_input == 'y'

    keep_oldest_input = input("–ü—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –æ—Å—Ç–∞–≤–ª—è—Ç—å —Å–∞–º—É—é —Å—Ç–∞—Ä—É—é –≤–µ—Ä—Å–∏—é? (y/n): ").lower()
    keep_oldest = keep_oldest_input == 'y'

    # –°–æ–∑–¥–∞–µ–º –æ—Ä–≥–∞–Ω–∏–∑–∞—Ç–æ—Ä
    organizer = FileOrganizer(source_folder, dry_run)

    print(f"\n{'=' * 40}")
    print("–ù–ê–ß–ò–ù–ê–Æ –û–†–ì–ê–ù–ò–ó–ê–¶–ò–Æ...")
    print(f"{'=' * 40}\n")

    # –ó–∞–ø—É—Å–∫–∞–µ–º –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—é
    try:
        success = organizer.organize_files()

        if success and remove_duplicates:
            print("\n" + "=" * 40)
            print("–ü–û–ò–°–ö –ò –£–î–ê–õ–ï–ù–ò–ï –î–£–ë–õ–ò–ö–ê–¢–û–í...")
            print("=" * 40)
            organizer.remove_duplicates(keep_oldest)

        # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        organizer.print_statistics()

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç
        if not dry_run:
            organizer.save_report()

        print("\n" + "=" * 40)
        if dry_run:
            print("‚úÖ –ü–†–ï–î–ü–†–û–°–ú–û–¢–† –ó–ê–í–ï–†–®–ï–ù")
            print("   –§–∞–π–ª—ã –Ω–µ –±—ã–ª–∏ –ø–µ—Ä–µ–º–µ—â–µ–Ω—ã.")
        else:
            print("‚úÖ –û–†–ì–ê–ù–ò–ó–ê–¶–ò–Ø –ó–ê–í–ï–†–®–ï–ù–ê")
            print(f"   –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {source_folder / 'organization_report.json'}")
        print("=" * 40)

    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  –û–ø–µ—Ä–∞—Ü–∏—è –ø—Ä–µ—Ä–≤–∞–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º.")
    except Exception as e:
        print(f"\n‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {e}")
        logging.exception("–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞:")


if __name__ == "__main__":
    main()